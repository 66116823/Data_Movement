import pandas as pd
import numpy as np
import os
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv1D, MaxPooling1D, LSTM, Dense, Dropout, BatchNormalization
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping

# ==========================================
# ‚öôÔ∏è 1. CONFIGURATION (‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏£‡∏∞‡∏ö‡∏ö)
# ==========================================
INPUT_FILE = r"E:\Data Movement\CollectDATA\Master_Data\Feature_Z-score_NormBase_WithFile.csv"
OUTPUT_MODEL_PATH = r"E:\Data Movement\CollectDATA\Master_Data\Best_CNN_LSTM_Hybrid.h5"

# ‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ Window
TIME_STEPS = 120  # ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ Input ‡∏¢‡∏≤‡∏ß 120 ‡πÄ‡∏ü‡∏£‡∏°
STEP_SIZE = 10  # üî• Downsampling: ‡∏Ç‡πâ‡∏≤‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡∏•‡∏∞ 10 ‡πÅ‡∏ñ‡∏ß (‡∏•‡∏î‡∏à‡∏≤‡∏Å 500Hz -> 50Hz)

# ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å Feature ‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÉ‡∏ä‡πâ (‡∏Ñ‡∏ß‡∏£‡πÉ‡∏ä‡πâ Z-score ‡πÅ‡∏•‡πâ‡∏ß)
# ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ‡∏ó‡∏±‡πâ‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏î‡∏¥‡∏ö‡πÅ‡∏•‡∏∞ Feature ‡∏ó‡∏µ‡πà‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏°‡∏≤ ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ CNN ‡∏à‡∏±‡∏ö Pattern ‡πÑ‡∏î‡πâ‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏™‡∏∏‡∏î
FEATURES = [
    'ACC_Feature_Z',
    'GYRO_Feature_Z',
    # ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå Z-score ‡∏Ç‡∏≠‡∏á‡πÅ‡∏Å‡∏ô‡∏î‡∏¥‡∏ö ‡πÉ‡∏´‡πâ‡πÉ‡∏™‡πà‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ï‡∏£‡∏á‡∏ô‡∏µ‡πâ‡∏à‡∏∞‡πÅ‡∏°‡πà‡∏ô‡∏Ç‡∏∂‡πâ‡∏ô‡∏Ñ‡∏£‡∏±‡∏ö ‡πÄ‡∏ä‡πà‡∏ô:
    # 'ACC_X_Z', 'ACC_Y_Z', 'ACC_Z_Z'
]


# ==========================================
# üõ†Ô∏è 2. DATA PIPELINE (‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•)
# ==========================================
def load_and_process_data(file_path, time_steps, feature_cols, step_size):
    print(f"üìÇ Loading Data from: {file_path}")
    if not os.path.exists(file_path):
        raise FileNotFoundError("‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå CSV ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡πÄ‡∏ä‡πá‡∏Ñ Path")

    df = pd.read_csv(file_path)

    # 1. ‡πÅ‡∏Å‡πâ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ã‡πâ‡∏≥ (Duplicate Rows)
    print(f"üßπ Cleaning duplicates...")
    original_len = len(df)
    df = df.drop_duplicates(subset=['Subject_ID', 'Filename', 'Time'])
    print(f"   - Reduced from {original_len} to {len(df)} rows")

    # 2. Group ‡∏ï‡∏≤‡∏°‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏´‡∏±‡πà‡∏ô‡πÄ‡∏õ‡πá‡∏ô‡∏ä‡∏¥‡πâ‡∏ô
    grouped = df.groupby(['Subject_ID', 'Filename'])

    sequences = []
    labels = []
    subject_ids = []

    print(f"‚è≥ Processing & Downsampling (Step={step_size})...")

    for (sub_id, fname), group in grouped:
        # 3. Downsampling (‡∏•‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•)
        # ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏°‡∏≤ 1 ‡πÅ‡∏ñ‡∏ß ‡∏ó‡∏∏‡∏Å‡πÜ 10 ‡πÅ‡∏ñ‡∏ß ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ç‡∏¢‡∏≤‡∏¢‡∏°‡∏∏‡∏°‡∏°‡∏≠‡∏á‡πÄ‡∏ß‡∏•‡∏≤‡πÉ‡∏´‡πâ‡∏Å‡∏ß‡πâ‡∏≤‡∏á‡∏Ç‡∏∂‡πâ‡∏ô
        group_ds = group.iloc[::step_size, :]

        series = group_ds[feature_cols].values

        # 4. Padding / Truncating (‡∏õ‡∏£‡∏±‡∏ö‡∏Ç‡∏ô‡∏≤‡∏î‡πÉ‡∏´‡πâ‡πÄ‡∏ó‡πà‡∏≤‡∏Å‡∏±‡∏ö 120)
        if len(series) == 0: continue

        if len(series) >= time_steps:
            seq = series[:time_steps]
        else:
            pad_len = time_steps - len(series)
            seq = np.pad(series, ((0, pad_len), (0, 0)), mode='constant')

        sequences.append(seq)
        labels.append(group['Label'].iloc[0])
        subject_ids.append(sub_id)

    print(f"‚úÖ Preprocessing Done! Got {len(sequences)} sequences.")
    return np.array(sequences), np.array(labels), np.array(subject_ids)


# --- ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô ---
X, y, subjects = load_and_process_data(INPUT_FILE, TIME_STEPS, FEATURES, STEP_SIZE)

# ==========================================
# ‚úÇÔ∏è 3. SUBJECT-BASED SPLITTING (‡πÅ‡∏ö‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏≤‡∏°‡∏Ñ‡∏ô)
# ==========================================
print("\nüîÑ Splitting Data (Subject-based)...")

unique_subjects = np.unique(subjects)
np.random.seed(42)
np.random.shuffle(unique_subjects)

# ‡∏™‡∏π‡∏ï‡∏£‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì: Test 20%, Val 10%, Train Rest
n_total = len(unique_subjects)
n_test = 5  # ‡∏ï‡∏≤‡∏°‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏Ñ‡∏∏‡∏¢‡∏Å‡∏±‡∏ô (‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì 20% ‡∏Ç‡∏≠‡∏á 24)
n_val = 2  # (‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì 10% ‡∏Ç‡∏≠‡∏á‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏•‡∏∑‡∏≠)
n_train = n_total - n_test - n_val

test_subs = unique_subjects[:n_test]
val_subs = unique_subjects[n_test: n_test + n_val]
train_subs = unique_subjects[n_test + n_val:]

print(f"   - Train Subjects ({len(train_subs)}): {train_subs}")
print(f"   - Val Subjects   ({len(val_subs)}): {val_subs}")
print(f"   - Test Subjects  ({len(test_subs)}): {test_subs}")

# ‡∏™‡∏£‡πâ‡∏≤‡∏á Mask ‡πÅ‡∏•‡∏∞‡πÅ‡∏ö‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
train_mask = np.isin(subjects, train_subs)
val_mask = np.isin(subjects, val_subs)
test_mask = np.isin(subjects, test_subs)

X_train, y_train = X[train_mask], y[train_mask]
X_val, y_val = X[val_mask], y[val_mask]
X_test, y_test = X[test_mask], y[test_mask]

# One-Hot Encoding Labels
num_classes = len(np.unique(y))
y_train_hot = to_categorical(y_train, num_classes)
y_val_hot = to_categorical(y_val, num_classes)
y_test_hot = to_categorical(y_test, num_classes)

# ==========================================
# üß† 4. BUILD CNN-LSTM HYBRID MODEL
# ==========================================
print("\nüèóÔ∏è Building CNN-LSTM Architecture...")

model = Sequential()

# --- Part 1: CNN (Feature Extractor) ---
# ‡∏ó‡∏≥‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà‡∏™‡πÅ‡∏Å‡∏ô‡∏´‡∏≤‡∏Å‡∏£‡∏≤‡∏ü‡∏ó‡∏µ‡πà‡∏ú‡∏¥‡∏î‡∏õ‡∏Å‡∏ï‡∏¥‡πÅ‡∏•‡∏∞‡∏•‡∏î Noise
model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(TIME_STEPS, len(FEATURES))))
model.add(BatchNormalization())  # ‡∏ä‡πà‡∏ß‡∏¢‡∏õ‡∏£‡∏±‡∏ö‡∏Ñ‡πà‡∏≤‡πÉ‡∏´‡πâ‡∏™‡∏°‡∏î‡∏∏‡∏• ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡πÑ‡∏ß‡∏Ç‡∏∂‡πâ‡∏ô
model.add(Conv1D(filters=64, kernel_size=3, activation='relu'))
model.add(BatchNormalization())

# MaxPooling: ‡∏¢‡πà‡∏≠‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏•‡∏á‡∏Ñ‡∏£‡∏∂‡πà‡∏á‡∏´‡∏ô‡∏∂‡πà‡∏á (‡∏à‡∏≤‡∏Å 120 ‡πÄ‡∏´‡∏•‡∏∑‡∏≠ 60)
model.add(MaxPooling1D(pool_size=2))
model.add(Dropout(0.3))

# --- Part 2: LSTM (Sequence Analyzer) ---
# ‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏¢‡πà‡∏≠‡πÅ‡∏•‡πâ‡∏ß ‡∏°‡∏≤‡∏î‡∏π‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ï‡πà‡∏≠‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏Ç‡∏≠‡∏á‡πÄ‡∏ß‡∏•‡∏≤
model.add(LSTM(100, return_sequences=False))
model.add(Dropout(0.4))

# --- Part 3: Classifier ---
model.add(Dense(64, activation='relu'))
model.add(Dense(num_classes, activation='softmax'))

model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
model.summary()

# ==========================================
# üî• 5. TRAINING
# ==========================================
callbacks = [
    ModelCheckpoint(OUTPUT_MODEL_PATH, save_best_only=True, monitor='val_accuracy', mode='max', verbose=1),
    EarlyStopping(monitor='val_loss', patience=15, verbose=1, restore_best_weights=True)
]

print("\nüöÄ Starting Training...")
history = model.fit(
    X_train, y_train_hot,
    epochs=60,  # ‡πÉ‡∏´‡πâ‡πÄ‡∏ß‡∏•‡∏≤‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏ô‡∏≤‡∏ô‡∏´‡∏ô‡πà‡∏≠‡∏¢
    batch_size=64,
    validation_data=(X_val, y_val_hot),
    callbacks=callbacks,
    verbose=1
)

# ==========================================
# üèÜ 6. FINAL EVALUATION
# ==========================================
print("\n" + "=" * 50)
print("üßê Evaluating on UNSEEN Test Set...")
loss, accuracy = model.evaluate(X_test, y_test_hot, verbose=0)
print(f"‚úÖ Test Accuracy: {accuracy * 100:.2f}%")
print("=" * 50)
