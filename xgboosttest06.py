import pandas as pd
import numpy as np
import os
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
import xgboost as xgb
import optuna
from optuna.pruners import MedianPruner
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.preprocessing import StandardScaler
from sklearn.utils import class_weight
from sklearn.model_selection import KFold
import matplotlib.pyplot as plt
import seaborn as sns

# ==========================================
# ‚öôÔ∏è 1. CONFIGURATION (‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡πÉ‡∏´‡∏°‡πà)
# ==========================================
# üìÇ ‡∏£‡∏∞‡∏ö‡∏∏‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏´‡∏°‡πà‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì
INPUT_FILE = r"D:\Data Movement\CollectDATA\Master_Data\Feature_Z-score_NormBase_1Sec.csv"

BASE_OUTPUT_PATH = r"D:\Data Movement\CollectDATA\Result_Model"
BASE_FOLDER_NAME = "XGBoost_LSTM_1Sec_Results"


# ---------------------------------------------------------
def get_next_output_dir(base_path, folder_name):
    i = 1
    while True:
        new_dir_name = f"{folder_name}_{i:02d}"
        full_path = os.path.join(base_path, new_dir_name)
        if not os.path.exists(full_path):
            return full_path
        i += 1


OUTPUT_DIR = get_next_output_dir(BASE_OUTPUT_PATH, BASE_FOLDER_NAME)
os.makedirs(OUTPUT_DIR, exist_ok=True)
print(f"‚úÖ New Output Directory Created: {OUTPUT_DIR}")
# ---------------------------------------------------------

# üî• [CRITICAL UPDATE] ‡∏Ñ‡πà‡∏≤‡∏Ñ‡∏á‡∏ó‡∏µ‡πà‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏£‡∏≤‡∏¢‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ
# 1 Row = 1 Second
# ‡πÄ‡∏£‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏î‡∏π‡πÄ‡∏´‡∏ï‡∏∏‡∏Å‡∏≤‡∏£‡∏ì‡πå‡∏¢‡πâ‡∏≠‡∏ô‡∏´‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì 6 ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡πÉ‡∏à
TIME_STEPS = 6
STEP_SIZE = 1  # ‡∏Ç‡∏¢‡∏±‡∏ö‡∏ó‡∏µ‡∏•‡∏∞ 1 ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ (‡∏´‡πâ‡∏≤‡∏°‡∏Å‡∏£‡∏∞‡πÇ‡∏î‡∏î‡∏Ç‡πâ‡∏≤‡∏°)
N_FOLDS = 10

FEATURES = [
    'ACC_X', 'ACC_Y', 'ACC_Z',
    'GYRO_X', 'GYRO_Y', 'GYRO_Z'
]

# ‡∏•‡∏î‡∏à‡∏≥‡∏ô‡∏ß‡∏ô Trials ‡∏•‡∏á‡∏ô‡∏¥‡∏î‡∏´‡∏ô‡πà‡∏≠‡∏¢‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ô‡πâ‡∏≠‡∏¢‡∏•‡∏á‡πÅ‡∏ï‡πà‡πÄ‡∏ô‡πâ‡∏ô‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û
N_TRIALS = 30
EPOCHS_PER_TRIAL = 50


# ==========================================
# üõ†Ô∏è 2. DATA LOADING & PROCESSING
# ==========================================
def load_and_process_data(file_path, time_steps, feature_cols, step_size):
    print(f"üìÇ Loading Data from: {file_path}")
    if not os.path.exists(file_path):
        raise FileNotFoundError("‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå CSV")

    df = pd.read_csv(file_path)

    # ‡∏Å‡∏£‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ã‡πâ‡∏≥
    print(f"üßπ Cleaning duplicates...")
    original_len = len(df)
    # ‡∏ï‡∏±‡∏î‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡πá‡∏Ñ‡∏ã‡πâ‡∏≥‡∏≠‡∏≠‡∏Å (‡πÄ‡∏ä‡πà‡∏ô Time ‡∏≠‡∏≤‡∏à‡∏à‡∏∞‡∏ï‡πà‡∏≤‡∏á‡∏Å‡∏±‡∏ô‡πÅ‡∏ï‡πà‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô)
    df = df.drop_duplicates(subset=['Subject_ID', 'Filename', 'Time'] + feature_cols)
    print(f"   - Reduced from {original_len} to {len(df)} rows")

    # üî• Normalization
    # ‡πÄ‡∏ä‡πá‡∏Ñ‡∏ß‡πà‡∏≤‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡πÑ‡∏´‡∏ô‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏ó‡∏≥ Z-score (‡∏à‡∏≤‡∏Å‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡∏Ñ‡∏≤‡∏î‡∏ß‡πà‡∏≤‡∏ó‡∏≥‡∏°‡∏≤‡πÅ‡∏•‡πâ‡∏ß‡∏ö‡∏≤‡∏á‡∏™‡πà‡∏ß‡∏ô ‡πÅ‡∏ï‡πà‡∏Å‡∏±‡∏ô‡πÄ‡∏´‡∏ô‡∏µ‡∏¢‡∏ß)
    # ‡∏™‡∏°‡∏°‡∏ï‡∏¥‡∏ß‡πà‡∏≤ Feature_Z ‡∏Ñ‡∏∑‡∏≠‡∏ó‡∏≥‡∏°‡∏≤‡πÅ‡∏•‡πâ‡∏ß, ‡πÅ‡∏ï‡πà Raw ACC/GYRO ‡∏≠‡∏≤‡∏à‡∏à‡∏∞‡∏¢‡∏±‡∏á
    raw_cols = [c for c in feature_cols if 'Feature_Z' not in c]
    if raw_cols:
        print(f"‚öñÔ∏è Normalizing raw columns: {raw_cols}")
        scaler = StandardScaler()
        df[raw_cols] = scaler.fit_transform(df[raw_cols])

    grouped = df.groupby(['Subject_ID', 'Filename'])
    sequences, labels, subject_ids = [], [], []

    print("‚úÇÔ∏è Slicing data into windows...")
    for (sub_id, fname), group in grouped:
        # Sort ‡∏ï‡∏≤‡∏°‡πÄ‡∏ß‡∏•‡∏≤‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á (‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏°‡∏≤‡∏Å‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏£‡∏≤‡∏¢‡∏ß‡∏¥)
        group = group.sort_values('Time')

        # ‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏∏‡∏Å‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î (‡πÄ‡∏û‡∏£‡∏≤‡∏∞ Step_size = 1)
        series = group[feature_cols].values
        label_series = group['Label'].values  # ‡πÄ‡∏Å‡πá‡∏ö Label ‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏ß‡∏¥‡πÑ‡∏ß‡πâ‡∏î‡πâ‡∏ß‡∏¢

        if len(series) < time_steps:
            # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏ü‡∏•‡πå‡∏™‡∏±‡πâ‡∏ô‡∏Å‡∏ß‡πà‡∏≤ 6 ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ ‡πÉ‡∏´‡πâ Pad (‡πÄ‡∏ï‡∏¥‡∏° 0 ‡∏Ç‡πâ‡∏≤‡∏á‡∏´‡∏ô‡πâ‡∏≤)
            pad_len = time_steps - len(series)
            series = np.pad(series, ((pad_len, 0), (0, 0)), mode='constant')
            # ‡πÉ‡∏ä‡πâ Label ‡∏ï‡∏±‡∏ß‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢‡∏Ç‡∏≠‡∏á‡πÑ‡∏ü‡∏•‡πå
            target_label = label_series[-1]

            sequences.append(series)
            labels.append(target_label)
            subject_ids.append(sub_id)
        else:
            # Sliding Window
            for i in range(0, len(series) - time_steps + 1, step_size):
                seq = series[i:i + time_steps]

                # Label: ‡πÉ‡∏ä‡πâ Label ‡∏Ç‡∏≠‡∏á‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢‡πÉ‡∏ô Window (‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡πÄ‡∏£‡∏≤‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢ ‡∏ì ‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô)
                # ‡∏´‡∏£‡∏∑‡∏≠‡πÉ‡∏ä‡πâ Mode (‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏û‡∏ö‡∏ö‡πà‡∏≠‡∏¢‡∏™‡∏∏‡∏î) ‡πÉ‡∏ô‡∏ä‡πà‡∏ß‡∏á 6 ‡∏ß‡∏¥
                # ‡πÉ‡∏ô‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πâ‡∏Ç‡∏≠‡πÉ‡∏ä‡πâ "‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢" ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° Real-time
                target_label = label_series[i + time_steps - 1]

                sequences.append(seq)
                labels.append(target_label)
                subject_ids.append(sub_id)

    # Handle NaNs in labels if any
    labels = np.nan_to_num(labels, nan=0).astype(int)

    print(f"‚úÖ Got {len(sequences)} sequences using {len(feature_cols)} features.")
    print(f"   (Window Size: {time_steps} sec, Step: {step_size} sec)")
    return np.array(sequences), np.array(labels), np.array(subject_ids)


# ==========================================
# üîß 3. FEATURE ENGINEERING FOR XGBOOST
# ==========================================
def extract_statistical_features(sequences):
    """
    ‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Time Series ‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥
    ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• 6 ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ: ‡∏Ñ‡πà‡∏≤ Diff ‡πÅ‡∏•‡∏∞ Slope ‡∏à‡∏∞‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏°‡∏≤‡∏Å
    """
    # print("üîß Extracting statistical features...") # ‡∏õ‡∏¥‡∏î print ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏•‡∏î‡∏£‡∏Å
    n_samples = sequences.shape[0]
    n_features = sequences.shape[2]
    feature_list = []

    for i in range(n_samples):
        features = []
        for f in range(n_features):
            series = sequences[i, :, f]

            # Basic Stats
            mean_val = np.mean(series)
            std_val = np.std(series)
            min_val = np.min(series)
            max_val = np.max(series)
            range_val = max_val - min_val

            # Dynamic Stats (‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡∏£‡∏≤‡∏¢‡∏ß‡∏¥)
            diff = np.diff(series)
            mean_change = np.mean(diff) if len(diff) > 0 else 0
            max_change = np.max(np.abs(diff)) if len(diff) > 0 else 0

            features.extend([mean_val, std_val, min_val, max_val, range_val, mean_change, max_change])

        feature_list.append(features)

    return np.array(feature_list)


def select_important_features_xgboost(X_stat, y, top_k=20):
    """‡πÉ‡∏ä‡πâ XGBoost ‡∏´‡∏≤‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç"""
    print(f"\nüéØ XGBoost Feature Selection (Top-{top_k})...")

    # ‡∏õ‡∏£‡∏±‡∏ö XGBoost ‡πÉ‡∏´‡πâ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏Å‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ô‡πâ‡∏≠‡∏¢‡∏•‡∏á (‡∏•‡∏î Depth ‡∏Å‡∏±‡∏ô Overfit)
    model = xgb.XGBClassifier(
        n_estimators=100, max_depth=4, learning_rate=0.05,
        random_state=42, verbosity=0,
        tree_method='hist'  # ‡πÄ‡∏£‡πá‡∏ß‡∏Ç‡∏∂‡πâ‡∏ô
    )
    model.fit(X_stat, y)
    importances = model.feature_importances_
    top_indices = np.argsort(importances)[-top_k:][::-1]
    print(f"‚úÖ Selected top {top_k} features.")
    return top_indices, model


def reconstruct_sequences_from_features(sequences, selected_indices, n_original_features):
    """Map index ‡∏Å‡∏•‡∏±‡∏ö‡πÑ‡∏õ‡∏ó‡∏µ‡πà Channel ‡πÄ‡∏î‡∏¥‡∏°"""
    # ‡πÄ‡∏£‡∏≤ extract 7 features ‡∏ï‡πà‡∏≠ 1 channel (mean, std, min, max, range, mean_change, max_change)
    features_per_channel = 7
    selected_channels = set()
    for idx in selected_indices:
        channel = idx // features_per_channel
        if channel < n_original_features:
            selected_channels.add(channel)

    selected_channels = sorted(list(selected_channels))

    # ‡∏Å‡∏±‡∏ô‡πÄ‡∏´‡∏ô‡∏µ‡∏¢‡∏ß: ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÄ‡∏•‡∏¢ ‡πÉ‡∏´‡πâ‡πÄ‡∏≠‡∏≤‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
    if not selected_channels:
        selected_channels = list(range(n_original_features))

    # print(f"   -> XGBoost Selected Channels: {selected_channels}")
    return sequences[:, :, selected_channels]


# ==========================================
# üß† 4. LSTM MODEL BUILDER
# ==========================================
def create_xgboost_lstm_model(trial, input_shape, num_classes):
    model = Sequential()

    # ‡∏õ‡∏£‡∏±‡∏ö LSTM Units ‡πÉ‡∏´‡πâ‡πÄ‡∏•‡πá‡∏Å‡∏•‡∏á‡∏´‡∏ô‡πà‡∏≠‡∏¢‡πÄ‡∏û‡∏£‡∏≤‡∏∞ Time step ‡∏ô‡πâ‡∏≠‡∏¢‡∏•‡∏á (6 steps)
    lstm_units = trial.suggest_categorical('lstm_units', [32, 64, 128])
    dropout_rate = trial.suggest_float('dropout_rate', 0.2, 0.5)

    model.add(LSTM(lstm_units, return_sequences=False, input_shape=input_shape))
    model.add(Dropout(dropout_rate))
    model.add(BatchNormalization())

    dense_units = trial.suggest_categorical('dense_units', [32, 64])
    model.add(Dense(dense_units, activation='relu'))
    model.add(Dense(num_classes, activation='softmax'))

    optimizer = tf.keras.optimizers.Adam(learning_rate=trial.suggest_float('learning_rate', 1e-4, 1e-2, log=True))
    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])
    return model


# ==========================================
# üéØ 5. OPTUNA OBJECTIVE
# ==========================================
def objective(trial, X_seq_train, y_train, X_seq_val, y_val, num_classes):
    # Parameter ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö XGBoost Selection
    top_k = trial.suggest_int('top_k_features', 10, 40, step=5)  # ‡∏•‡∏î‡∏à‡∏≥‡∏ô‡∏ß‡∏ô feature ‡∏•‡∏á‡πÄ‡∏û‡∏£‡∏≤‡∏∞ input ‡∏ô‡πâ‡∏≠‡∏¢

    X_stat_train = extract_statistical_features(X_seq_train)
    y_train_lbl = np.argmax(y_train, axis=1)

    selected_indices, _ = select_important_features_xgboost(X_stat_train, y_train_lbl, top_k=top_k)

    n_features_orig = len(FEATURES)
    X_train_reduced = reconstruct_sequences_from_features(X_seq_train, selected_indices, n_features_orig)
    X_val_reduced = reconstruct_sequences_from_features(X_seq_val, selected_indices, n_features_orig)

    input_shape = (X_train_reduced.shape[1], X_train_reduced.shape[2])
    model = create_xgboost_lstm_model(trial, input_shape, num_classes)

    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

    history = model.fit(
        X_train_reduced, y_train,
        validation_data=(X_val_reduced, y_val),
        epochs=EPOCHS_PER_TRIAL,
        batch_size=trial.suggest_categorical('batch_size', [16, 32]),  # Batch ‡πÄ‡∏•‡πá‡∏Å‡∏•‡∏á
        callbacks=[early_stopping, optuna.integration.TFKerasPruningCallback(trial, 'val_accuracy')],
        verbose=0
    )
    return max(history.history['val_accuracy'])


# ==========================================
# üß† 6. TRAIN FINAL MODEL
# ==========================================
def train_final_model(model, X_train, y_train, X_val, y_val, params):
    print("\nüî® Training Final Model...")
    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, verbose=1)
    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=4, min_lr=1e-6, verbose=1)

    y_integers = np.argmax(y_train, axis=1)
    weights = class_weight.compute_class_weight('balanced', classes=np.unique(y_integers), y=y_integers)
    class_weights_dict = dict(enumerate(weights))
    print(f"‚öñÔ∏è Class Weights: {class_weights_dict}")

    history = model.fit(
        X_train, y_train,
        validation_data=(X_val, y_val),
        epochs=100,
        batch_size=params['batch_size'],
        callbacks=[early_stopping, reduce_lr],
        class_weight=class_weights_dict,
        verbose=1
    )
    return history


# ==========================================
# üöÄ 7. MAIN EXECUTION WITH K-FOLD CV (UPDATED: Tune Every Fold)
# ==========================================
if __name__ == "__main__":
    print("=" * 60)
    print("üî¨ XGBoost-LSTM (1-Sec Resolution) with Full K-Fold Optimization")
    print("=" * 60)

    # 1. Load Data
    # ‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏: ‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤ TIME_STEPS = 6, STEP_SIZE = 1 ‡∏ï‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏¢‡∏Å‡∏±‡∏ô
    X, y, subjects = load_and_process_data(INPUT_FILE, TIME_STEPS, FEATURES, step_size=STEP_SIZE)

    num_classes = len(np.unique(y))
    print(f"\nüìä Classes found: {np.unique(y)} (Total: {num_classes})")

    unique_subjects = np.unique(subjects)
    # np.random.seed(42) # ‡πÄ‡∏õ‡∏¥‡∏î‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏ô‡∏µ‡πâ‡∏ñ‡πâ‡∏≤‡∏≠‡∏¢‡∏≤‡∏Å‡πÉ‡∏´‡πâ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡πÅ‡∏ö‡πà‡∏á Fold ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏î‡∏¥‡∏°‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á
    np.random.shuffle(unique_subjects)

    print(f"\nüë• Total Subjects Found: {len(unique_subjects)}")

    kfold = KFold(n_splits=N_FOLDS, shuffle=False)
    fold_results = []
    fold_predictions = []
    fold_true_labels = []

    # Loop Folds
    for fold_idx, (train_val_indices, test_indices) in enumerate(kfold.split(unique_subjects)):
        print("\n" + "=" * 80)
        print(f"üìå FOLD {fold_idx + 1}/{N_FOLDS}")
        print("=" * 80)

        train_val_subs = unique_subjects[train_val_indices]
        test_subs = unique_subjects[test_indices]

        # Validation Split
        n_val = max(1, int(len(train_val_subs) * 0.15))
        val_subs = train_val_subs[:n_val]
        train_subs = train_val_subs[n_val:]

        train_mask = np.isin(subjects, train_subs)
        val_mask = np.isin(subjects, val_subs)
        test_mask = np.isin(subjects, test_subs)

        X_train, y_train = X[train_mask], y[train_mask]
        X_val, y_val = X[val_mask], y[val_mask]
        X_test, y_test = X[test_mask], y[test_mask]

        y_train_hot = to_categorical(y_train, num_classes)
        y_val_hot = to_categorical(y_val, num_classes)
        y_test_hot = to_categorical(y_test, num_classes)

        # ----------------------------------------------------------------
        # üî• UPDATED: ‡∏à‡∏π‡∏ô Optuna ‡πÉ‡∏´‡∏°‡πà "‡∏ó‡∏∏‡∏Å Fold" (‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ if fold_idx == 0 ‡πÅ‡∏•‡πâ‡∏ß)
        # ----------------------------------------------------------------
        print(f"\nüîç [Fold {fold_idx + 1}] Starting Optuna Optimization ({N_TRIALS} trials)...")

        # ‡∏™‡∏£‡πâ‡∏≤‡∏á Study ‡πÉ‡∏´‡∏°‡πà‡∏ó‡∏∏‡∏Å‡∏£‡∏≠‡∏ö ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ‡∏à‡∏≥‡∏Ñ‡πà‡∏≤‡∏à‡∏≤‡∏Å Fold ‡πÄ‡∏Å‡πà‡∏≤
        study = optuna.create_study(direction='maximize', pruner=MedianPruner())
        study.optimize(lambda trial: objective(trial, X_train, y_train_hot, X_val, y_val_hot, num_classes),
                       n_trials=N_TRIALS)

        best_params = study.best_params
        print(f"üèÜ [Fold {fold_idx + 1}] Best Params Found: {best_params}")
        print(f"   Best Val Accuracy: {study.best_value:.4f}")

        # ----------------------------------------------------------------
        # Feature Selection & Training using THIS FOLD's best params
        # ----------------------------------------------------------------
        X_stat_train = extract_statistical_features(X_train)
        selected_indices, _ = select_important_features_xgboost(
            X_stat_train, np.argmax(y_train_hot, axis=1), top_k=best_params['top_k_features']
        )

        X_train_final = reconstruct_sequences_from_features(X_train, selected_indices, len(FEATURES))
        X_val_final = reconstruct_sequences_from_features(X_val, selected_indices, len(FEATURES))
        X_test_final = reconstruct_sequences_from_features(X_test, selected_indices, len(FEATURES))


        # Helper class for fixed params
        class FixedTrial:
            def suggest_categorical(self, n, c): return best_params[n]

            def suggest_float(self, n, l, h, log=False): return best_params[n]


        fold_model = create_xgboost_lstm_model(FixedTrial(), (X_train_final.shape[1], X_train_final.shape[2]),
                                               num_classes)
        train_final_model(fold_model, X_train_final, y_train_hot, X_val_final, y_val_hot, best_params)

        # Evaluate
        print(f"\nüìä Evaluating Fold {fold_idx + 1}...")
        y_pred = np.argmax(fold_model.predict(X_test_final), axis=1)
        y_true = np.argmax(y_test_hot, axis=1)

        fold_predictions.extend(y_pred)
        fold_true_labels.extend(y_true)

        report = classification_report(y_true, y_pred, output_dict=True, zero_division=0)
        fold_results.append(report)
        print(f"‚úÖ Fold {fold_idx + 1} Test Acc: {report['accuracy']:.4f}")

        # Save Fold Model
        fold_model.save(os.path.join(OUTPUT_DIR, f"Fold_{fold_idx + 1}_Model.h5"))

        # Save Confusion Matrix per Fold
        cm = confusion_matrix(y_true, y_pred)
        plt.figure(figsize=(6, 5))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
        plt.title(f"Confusion Matrix - Fold {fold_idx + 1}")
        plt.savefig(os.path.join(OUTPUT_DIR, f"confusion_matrix_fold_{fold_idx + 1}.png"))
        plt.close()

    # ==========================================
    # Summary of All Folds
    # ==========================================
    overall_acc = np.mean([f['accuracy'] for f in fold_results])
    print("\n" + "=" * 80)
    print(f"üéØ OVERALL ACCURACY ({N_FOLDS}-Fold CV): {overall_acc:.4f}")
    print("=" * 80)

    # Save Results
    results_df = pd.DataFrame(classification_report(fold_true_labels, fold_predictions, output_dict=True)).transpose()
    results_df.to_csv(os.path.join(OUTPUT_DIR, "overall_results.csv"))
    print("\n--- Overall Report ---")
    print(results_df)

    cm = confusion_matrix(fold_true_labels, fold_predictions)
    plt.figure(figsize=(10, 8))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
    plt.title(f"Overall Confusion Matrix ({N_FOLDS}-Fold CV)")
    plt.savefig(os.path.join(OUTPUT_DIR, "overall_confusion_matrix.png"))
    print(f"\n‚úÖ All Done! Results saved in: {OUTPUT_DIR}")