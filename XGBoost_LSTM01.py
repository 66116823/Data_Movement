import pandas as pd
import numpy as np
import os
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.callbacks import EarlyStopping
import xgboost as xgb
import optuna
from optuna.pruners import MedianPruner
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
import matplotlib.pyplot as plt
import seaborn as sns

# ==========================================
# ‚öôÔ∏è 1. CONFIGURATION
# ==========================================
INPUT_FILE = r"E:\Data Movement\CollectDATA\Master_Data\Feature_Z-score_NormBase_WithFile.csv"
OUTPUT_DIR = r"E:\Data Movement\CollectDATA\Master_Data\XGBoost_LSTM_Results"
os.makedirs(OUTPUT_DIR, exist_ok=True)

# ‡∏Ñ‡πà‡∏≤‡∏Ñ‡∏á‡∏ó‡∏µ‡πà
TIME_STEPS = 120  # ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏°‡∏≠‡∏á‡∏¢‡πâ‡∏≠‡∏ô‡∏´‡∏•‡∏±‡∏á (Sequence Length)

# ‡πÉ‡∏™‡πà‡∏ó‡∏±‡πâ‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏î‡∏¥‡∏ö (‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏à‡∏±‡∏ö Dunk) ‡πÅ‡∏•‡∏∞‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Normalized
FEATURES = [
    'ACC_X', 'ACC_Y', 'ACC_Z',
    'GYRO_X', 'GYRO_Y', 'GYRO_Z',
    'ACC_Feature_Z', 'GYRO_Feature_Z'
]

# ‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ Optuna
N_TRIALS = 30
EPOCHS_PER_TRIAL = 25


# ==========================================
# üõ†Ô∏è 2. DATA LOADING & PROCESSING
# ==========================================
def load_and_process_data(file_path, time_steps, feature_cols, step_size):
    """‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•"""
    print(f"üìÇ Loading Data from: {file_path}")
    if not os.path.exists(file_path):
        raise FileNotFoundError("‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå CSV")

    df = pd.read_csv(file_path)

    print(f"üßπ Cleaning duplicates...")
    original_len = len(df)

    # üî• [‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç 2] ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£‡∏•‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ã‡πâ‡∏≥‡πÉ‡∏´‡πâ‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢‡∏Ç‡∏∂‡πâ‡∏ô
    # ‡πÑ‡∏°‡πà‡∏™‡∏ô‡πÉ‡∏à‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå Time ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡πá‡∏Ñ‡∏ã‡πâ‡∏≥ (‡πÄ‡∏ú‡∏∑‡πà‡∏≠ Excel ‡∏¢‡∏±‡∏á‡∏°‡∏µ‡∏õ‡∏±‡∏ç‡∏´‡∏≤)
    # ‡πÅ‡∏ï‡πà‡πÄ‡∏ä‡πá‡∏Ñ‡∏ß‡πà‡∏≤‡∏ñ‡πâ‡∏≤ "‡∏Ñ‡πà‡∏≤‡πÄ‡∏ã‡∏ô‡πÄ‡∏ã‡∏≠‡∏£‡πå‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î" ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏Å‡∏±‡∏ô‡πÄ‡∏õ‡πä‡∏∞‡πÜ ‡∏Ñ‡πà‡∏≠‡∏¢‡∏•‡∏ö
    df = df.drop_duplicates(subset=['Subject_ID', 'Filename'] + feature_cols)

    print(f"   - Reduced from {original_len} to {len(df)} rows")
    print(f"   - Note: Keeping rows even if timestamps look duplicate, provided sensor data is different.")

    grouped = df.groupby(['Subject_ID', 'Filename'])
    sequences, labels, subject_ids = [], [], []

    for (sub_id, fname), group in grouped:
        # Downsampling (‡∏ñ‡πâ‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏•‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏µ‡πà‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ‡πÉ‡∏´‡πâ‡∏õ‡∏£‡∏±‡∏ö step_size > 1)
        group_ds = group.iloc[::step_size, :]

        series = group_ds[feature_cols].values

        if len(series) == 0:
            continue

        # Padding / Truncating ‡πÉ‡∏´‡πâ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡πÄ‡∏ó‡πà‡∏≤‡∏Å‡∏±‡∏ö TIME_STEPS
        if len(series) >= time_steps:
            seq = series[:time_steps]
        else:
            pad_len = time_steps - len(series)
            seq = np.pad(series, ((0, pad_len), (0, 0)), mode='constant')

        sequences.append(seq)

        # ‡πÉ‡∏ä‡πâ Label ‡∏ï‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏°‡∏µ‡πÉ‡∏ô‡πÑ‡∏ü‡∏•‡πå (0, 1, 2)
        try:
            labels.append(group['Label'].iloc[0])
        except KeyError:
            # ‡∏Å‡∏£‡∏ì‡∏µ‡∏´‡∏≤ Label ‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠ ‡∏•‡∏≠‡∏á‡∏´‡∏≤ Scenario ‡πÅ‡∏•‡πâ‡∏ß Map ‡πÄ‡∏≠‡∏≤ (Optional)
            labels.append(0)

        subject_ids.append(sub_id)

    print(f"‚úÖ Got {len(sequences)} sequences using 6-axis features.")
    return np.array(sequences), np.array(labels), np.array(subject_ids)


# ==========================================
# üîß 3. FEATURE ENGINEERING FOR XGBOOST
# ==========================================
def extract_statistical_features(sequences):
    """‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Time Series (3D) ‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥ (2D) ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ XGBoost ‡∏≠‡πà‡∏≤‡∏ô‡∏£‡∏π‡πâ‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á"""
    print("üîß Extracting statistical features...")
    n_samples = sequences.shape[0]
    n_features = sequences.shape[2]

    feature_list = []

    for i in range(n_samples):
        features = []
        for f in range(n_features):  # ‡∏ß‡∏ô‡∏•‡∏π‡∏õ‡∏ó‡∏µ‡∏•‡∏∞‡πÅ‡∏Å‡∏ô (ACC_X, ..., GYRO_Z)
            series = sequences[i, :, f]

            # ‡∏Ñ‡πà‡∏≤‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡∏ï‡πà‡∏≤‡∏á‡πÜ ‡∏ó‡∏µ‡πà‡∏™‡∏∞‡∏ó‡πâ‡∏≠‡∏ô‡∏•‡∏±‡∏Å‡∏©‡∏ì‡∏∞‡∏Å‡∏≤‡∏£‡πÄ‡∏î‡∏¥‡∏ô
            features.extend([
                np.mean(series),  # ‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢
                np.std(series),  # ‡∏™‡πà‡∏ß‡∏ô‡πÄ‡∏ö‡∏µ‡πà‡∏¢‡∏á‡πÄ‡∏ö‡∏ô‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô (‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Dunk)
                np.min(series),  # ‡∏Ñ‡πà‡∏≤‡∏ï‡πà‡∏≥‡∏™‡∏∏‡∏î
                np.max(series),  # ‡∏Ñ‡πà‡∏≤‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î
                np.max(series) - np.min(series),  # Range (‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Å‡∏ß‡πâ‡∏≤‡∏á‡∏Ç‡∏≠‡∏á‡∏Ñ‡πà‡∏≤)
                np.var(series),  # ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏õ‡∏£‡∏õ‡∏£‡∏ß‡∏ô
            ])

            # ‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á (Jerk/Change)
            diff = np.diff(series)
            features.extend([
                np.mean(np.abs(diff)),  # ‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢
                np.max(np.abs(diff)),  # ‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏∞‡∏ä‡∏≤‡∏Å‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î
            ])

        feature_list.append(features)

    X_stat = np.array(feature_list)
    return X_stat


def select_important_features_xgboost(X_stat, y, top_k=50):
    """‡πÉ‡∏ä‡πâ XGBoost ‡∏´‡∏≤‡∏ß‡πà‡∏≤‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå‡πÑ‡∏´‡∏ô‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î"""
    print(f"\nüéØ XGBoost Feature Selection (Top-{top_k})...")

    model = xgb.XGBClassifier(
        n_estimators=100, max_depth=5, learning_rate=0.1,
        random_state=42, verbosity=0
    )
    model.fit(X_stat, y)

    importances = model.feature_importances_
    # ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å Index ‡∏Ç‡∏≠‡∏á‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î
    top_indices = np.argsort(importances)[-top_k:][::-1]

    print(f"‚úÖ Selected top {top_k} features.")
    return top_indices, model


def reconstruct_sequences_from_features(sequences, selected_indices, n_original_features):
    """‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÅ‡∏Å‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (Channel) ‡∏ó‡∏µ‡πà XGBoost ‡∏ö‡∏≠‡∏Å‡∏ß‡πà‡∏≤‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç ‡∏°‡∏≤‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÄ‡∏õ‡πá‡∏ô Sequence ‡πÉ‡∏´‡∏°‡πà"""
    # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏à‡∏≥‡∏ô‡∏ß‡∏ô Stats ‡∏ï‡πà‡∏≠ 1 Channel (‡∏ï‡∏≤‡∏°‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô extract_statistical_features)
    # ‡πÄ‡∏£‡∏≤‡πÉ‡∏™‡πà‡πÑ‡∏õ 6 stats + 2 diff stats = 8 features ‡∏ï‡πà‡∏≠‡πÅ‡∏Å‡∏ô
    features_per_channel = 8

    selected_channels = set()
    for idx in selected_indices:
        channel = idx // features_per_channel
        if channel < n_original_features:
            selected_channels.add(channel)

    selected_channels = sorted(list(selected_channels))

    # ‡∏Å‡∏±‡∏ô‡πÄ‡∏´‡∏ô‡∏µ‡∏¢‡∏ß: ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡πÄ‡∏´‡∏•‡∏∑‡∏≠ Channel ‡πÄ‡∏•‡∏¢ ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
    if not selected_channels:
        selected_channels = range(n_original_features)

    print(f"   -> Retained Channels Indices: {selected_channels}")

    # ‡∏ï‡∏±‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏≠‡∏≤‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÅ‡∏Å‡∏ô‡∏ó‡∏µ‡πà‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç
    sequences_reduced = sequences[:, :, selected_channels]
    return sequences_reduced


# ==========================================
# üß† 4. LSTM MODEL BUILDER
# ==========================================
def create_xgboost_lstm_model(trial, input_shape, num_classes):
    """‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á LSTM ‡πÇ‡∏î‡∏¢‡πÉ‡∏´‡πâ Optuna ‡∏ä‡πà‡∏ß‡∏¢‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Ñ‡πà‡∏≤"""

    model = Sequential()

    # LSTM Layers
    lstm_units_1 = trial.suggest_categorical('lstm_units_1', [64, 128, 200])
    dropout_rate = trial.suggest_float('dropout_rate', 0.2, 0.5)

    model.add(LSTM(lstm_units_1, return_sequences=False, input_shape=input_shape))
    model.add(Dropout(dropout_rate))
    model.add(BatchNormalization())

    # Dense Layers
    dense_units = trial.suggest_categorical('dense_units', [32, 64, 128])
    model.add(Dense(dense_units, activation='relu'))
    model.add(Dense(num_classes, activation='softmax'))  # Output Layer

    optimizer = tf.keras.optimizers.Adam(learning_rate=trial.suggest_float('learning_rate', 1e-4, 1e-2, log=True))
    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])

    return model


# ==========================================
# üéØ 5. OPTUNA OBJECTIVE
# ==========================================
def objective(trial, X_seq_train, y_train, X_seq_val, y_val, num_classes):
    # 1. XGBoost Selection Phase
    top_k = trial.suggest_int('top_k_features', 20, 48, step=4)  # ‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î 6 ‡πÅ‡∏Å‡∏ô * 8 stats = 48

    X_stat_train = extract_statistical_features(X_seq_train)

    # ‡πÅ‡∏õ‡∏•‡∏á One-hot ‡∏Å‡∏•‡∏±‡∏ö‡πÄ‡∏õ‡πá‡∏ô Class ‡∏õ‡∏Å‡∏ï‡∏¥‡∏ä‡∏±‡πà‡∏ß‡∏Ñ‡∏£‡∏≤‡∏ß‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏ó‡∏£‡∏ô XGBoost
    y_train_lbl = np.argmax(y_train, axis=1)

    selected_indices, _ = select_important_features_xgboost(X_stat_train, y_train_lbl, top_k=top_k)

    # 2. Reconstruct Data for LSTM
    n_features_orig = len(FEATURES)
    X_train_reduced = reconstruct_sequences_from_features(X_seq_train, selected_indices, n_features_orig)
    X_val_reduced = reconstruct_sequences_from_features(X_seq_val, selected_indices, n_features_orig)

    # 3. Train LSTM
    input_shape = (X_train_reduced.shape[1], X_train_reduced.shape[2])
    model = create_xgboost_lstm_model(trial, input_shape, num_classes)

    early_stopping = EarlyStopping(monitor='val_loss', patience=4, restore_best_weights=True)

    history = model.fit(
        X_train_reduced, y_train,
        validation_data=(X_val_reduced, y_val),
        epochs=EPOCHS_PER_TRIAL,
        batch_size=trial.suggest_categorical('batch_size', [32, 64]),
        callbacks=[early_stopping, optuna.integration.TFKerasPruningCallback(trial, 'val_accuracy')],
        verbose=0
    )

    return max(history.history['val_accuracy'])


# ==========================================
# üöÄ 6. MAIN EXECUTION
# ==========================================
if __name__ == "__main__":
    print("=" * 60)
    print("üî¨ XGBoost-LSTM Hybrid Model (Real-Time Optimized)")
    print("=" * 60)

    # 1. Load Data
    # step_size=1 ‡∏Ñ‡∏∑‡∏≠‡πÄ‡∏≠‡∏≤‡∏ó‡∏∏‡∏Å millisecond ‡∏ó‡∏µ‡πà‡∏°‡∏µ (‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏™‡∏∏‡∏î) ‡∏ñ‡πâ‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏¢‡∏≠‡∏∞‡πÄ‡∏Å‡∏¥‡∏ô‡∏Ñ‡πà‡∏≠‡∏¢‡∏õ‡∏£‡∏±‡∏ö‡πÄ‡∏õ‡πá‡∏ô 2-5
    X, y, subjects = load_and_process_data(INPUT_FILE, TIME_STEPS, FEATURES, step_size=1)

    num_classes = len(np.unique(y))
    print(f"\nüìä Classes found: {np.unique(y)} (Total: {num_classes})")

    # 2. Split Data (Subject-Independent)
    unique_subjects = np.unique(subjects)
    np.random.seed(42)
    np.random.shuffle(unique_subjects)

    n_test = 5
    n_val = 2

    test_subs = unique_subjects[:n_test]
    val_subs = unique_subjects[n_test:n_test + n_val]
    train_subs = unique_subjects[n_test + n_val:]

    train_mask = np.isin(subjects, train_subs)
    val_mask = np.isin(subjects, val_subs)
    test_mask = np.isin(subjects, test_subs)

    X_train, y_train = X[train_mask], y[train_mask]
    X_val, y_val = X[val_mask], y[val_mask]
    X_test, y_test = X[test_mask], y[test_mask]

    y_train_hot = to_categorical(y_train, num_classes)
    y_val_hot = to_categorical(y_val, num_classes)
    y_test_hot = to_categorical(y_test, num_classes)

    # 3. Optuna Tuning
    print(f"\nüîç Starting Optimization ({N_TRIALS} trials)...")
    study = optuna.create_study(direction='maximize', pruner=MedianPruner())
    study.optimize(lambda trial: objective(trial, X_train, y_train_hot, X_val, y_val_hot, num_classes),
                   n_trials=N_TRIALS)

    print(f"\nüèÜ Best Accuracy: {study.best_value:.4f}")
    print(f"   Best Params: {study.best_params}")

    # 4. Train Final Model
    print("\nüî® Training Final Model...")
    best_params = study.best_params

    # ‡∏ó‡∏≥‡∏ã‡πâ‡∏≥‡∏Å‡∏£‡∏∞‡∏ö‡∏ß‡∏ô‡∏Å‡∏≤‡∏£‡∏Ñ‡∏±‡∏î‡πÄ‡∏•‡∏∑‡∏≠‡∏Å Feature ‡∏î‡πâ‡∏ß‡∏¢ Best Params
    X_stat_train = extract_statistical_features(X_train)
    selected_indices, xgb_model = select_important_features_xgboost(
        X_stat_train, np.argmax(y_train_hot, axis=1), top_k=best_params['top_k_features']
    )

    X_train_final = reconstruct_sequences_from_features(X_train, selected_indices, len(FEATURES))
    X_val_final = reconstruct_sequences_from_features(X_val, selected_indices, len(FEATURES))
    X_test_final = reconstruct_sequences_from_features(X_test, selected_indices, len(FEATURES))


    # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢
    class FixedTrial:
        def suggest_categorical(self, n, c): return best_params[n]

        def suggest_float(self, n, l, h, log=False): return best_params[n]


    final_model = create_xgboost_lstm_model(FixedTrial(), (X_train_final.shape[1], X_train_final.shape[2]), num_classes)

    final_model.fit(
        X_train_final, y_train_hot,
        validation_data=(X_val_final, y_val_hot),
        epochs=50,
        batch_size=best_params['batch_size'],
        callbacks=[EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)],
        verbose=1
    )

    final_model.save(os.path.join(OUTPUT_DIR, "Final_XGB_LSTM_Model.h5"))

    # 5. Evaluate
    print("\n" + "=" * 60)
    print("üìä FINAL TEST RESULTS")
    print("=" * 60)
    y_pred = np.argmax(final_model.predict(X_test_final), axis=1)
    y_true = np.argmax(y_test_hot, axis=1)

    print(classification_report(y_true, y_pred))

    # Save Confusion Matrix
    cm = confusion_matrix(y_true, y_pred)
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
    plt.title("Confusion Matrix")
    plt.savefig(os.path.join(OUTPUT_DIR, "confusion_matrix.png"))
    print("‚úÖ Done!")