import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import os
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.preprocessing import StandardScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, LSTM, Conv1D, MaxPooling1D
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.utils import to_categorical
# üî• ‡πÄ‡∏û‡∏¥‡πà‡∏° EarlyStopping ‡πÄ‡∏Ç‡πâ‡∏≤‡∏°‡∏≤
from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping 

# ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ data_loader ‡∏ï‡∏±‡∏ß‡πÉ‡∏´‡∏°‡πà‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì (‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡πÅ‡∏õ‡∏•‡∏á BMI)
from data_loader import load_data

# ==========================================
# 1. CONFIGURATION
# ==========================================
ROOT_DIR = r"D:\Data Movement\CollectDATA\Subject"
OUTPUT_DIR = r"D:\Data Movement\CollectDATA\Model"

WINDOW_SIZE = 128
STEP_SIZE = 64
BATCH_SIZE = 32
# üî• ‡∏ï‡∏±‡πâ‡∏á Epoch ‡πÑ‡∏ß‡πâ‡∏™‡∏π‡∏á‡πÜ ‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢ ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡πÄ‡∏£‡∏≤‡∏°‡∏µ‡∏ï‡∏±‡∏ß‡∏ï‡∏±‡∏î‡∏à‡∏ö‡πÅ‡∏•‡πâ‡∏ß
EPOCHS = 100           
LEARNING_RATE = 0.001
CLASSES = ['Low Risk', 'Medium Risk', 'High Risk']

if not os.path.exists(OUTPUT_DIR):
    os.makedirs(OUTPUT_DIR)

# ==========================================
# 2. DATA PREPARATION
# ==========================================
print("üîÑ Loading Data (with BMI Categories 0,1,2)...")
X, y = load_data(ROOT_DIR, WINDOW_SIZE, STEP_SIZE)

if len(X) == 0:
    print("‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•! ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Path")
    exit()

# ‡πÅ‡∏õ‡∏•‡∏á Label ‡πÄ‡∏õ‡πá‡∏ô One-Hot
y_onehot = to_categorical(y, num_classes=3)

# ‡πÅ‡∏ö‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (Stratified ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏™‡∏±‡∏î‡∏™‡πà‡∏ß‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏Ñ‡∏•‡∏≤‡∏™‡πÄ‡∏ó‡πà‡∏≤‡πÄ‡∏î‡∏¥‡∏°)
X_train, X_test, y_train, y_test = train_test_split(
    X, y_onehot, test_size=0.2, random_state=42, stratify=y
)

print(f"üìä Training Data Shape: {X_train.shape}")

# ==========================================
# 3. NORMALIZATION (‡∏¢‡∏±‡∏á‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏≠‡∏¢‡∏π‡πà)
# ==========================================
# ‡πÅ‡∏°‡πâ BMI ‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô 0,1,2 ‡πÅ‡∏•‡πâ‡∏ß ‡πÅ‡∏ï‡πà‡πÄ‡∏£‡∏≤‡∏Ñ‡∏ß‡∏£‡∏õ‡∏£‡∏±‡∏ö‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ö Sensor (Mean=0, Std=1)
scaler = StandardScaler()

N_train, T, F = X_train.shape
X_train_reshaped = X_train.reshape(-1, F)
X_train_scaled = scaler.fit_transform(X_train_reshaped)
X_train = X_train_scaled.reshape(N_train, T, F)

# ‡∏õ‡∏£‡∏±‡∏ö Test set ‡∏î‡πâ‡∏ß‡∏¢ scaler ‡∏Ç‡∏≠‡∏á Train
N_test, T, F = X_test.shape
X_test_reshaped = X_test.reshape(-1, F)
X_test = scaler.transform(X_test_reshaped).reshape(N_test, T, F)

print("‚úÖ Data Normalized ‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢")

# ==========================================
# 4. BUILD MODEL
# ==========================================
model = Sequential()

# CNN Layers
model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(WINDOW_SIZE, F)))
model.add(Conv1D(filters=64, kernel_size=3, activation='relu'))
model.add(Dropout(0.5))
model.add(MaxPooling1D(pool_size=2))

# LSTM Layers
model.add(LSTM(100, return_sequences=False))
model.add(Dropout(0.5))

# Output Layers
model.add(Dense(100, activation='relu'))
model.add(Dense(3, activation='softmax'))

model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=LEARNING_RATE), metrics=['accuracy'])

# ==========================================
# 5. CALLBACKS (‡∏û‡∏£‡∏∞‡πÄ‡∏≠‡∏Å‡∏Ç‡∏≠‡∏á‡πÄ‡∏£‡∏≤)
# ==========================================
# 1. ‡∏•‡∏î Learning Rate ‡∏ñ‡πâ‡∏≤‡∏£‡∏≤‡∏ö‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ
reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=0.00001, verbose=1)

# 2. üî• Early Stopping: ‡∏ñ‡πâ‡∏≤‡πÄ‡∏ó‡∏£‡∏ô‡πÑ‡∏õ 15 ‡∏£‡∏≠‡∏ö‡πÅ‡∏•‡πâ‡∏ß Val Loss ‡πÑ‡∏°‡πà‡∏î‡∏µ‡∏Ç‡∏∂‡πâ‡∏ô -> ‡∏™‡∏±‡πà‡∏á‡∏´‡∏¢‡∏∏‡∏î! ‡πÅ‡∏•‡∏∞‡∏¢‡πâ‡∏≠‡∏ô‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î
early_stop = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True, verbose=1)

# ==========================================
# 6. TRAIN MODEL
# ==========================================
print("\nüöÄ START TRAINING (with Early Stopping)...")
history = model.fit(
    X_train, y_train, 
    epochs=EPOCHS, 
    batch_size=BATCH_SIZE, 
    validation_data=(X_test, y_test), 
    callbacks=[reduce_lr, early_stop],  # ‡πÉ‡∏™‡πà‡∏ó‡∏±‡πâ‡∏á‡∏Ñ‡∏π‡πà‡πÄ‡∏•‡∏¢
    verbose=1
)

# ==========================================
# 7. EVALUATION
# ==========================================
print("\nüìù SAVING RESULTS (V3)...")

# ‡∏Å‡∏£‡∏≤‡∏ü
plt.figure(figsize=(12, 4))
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Train Acc')
plt.plot(history.history['val_accuracy'], label='Val Acc')
plt.title('Model Accuracy (BMI Categorized)')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Val Loss')
plt.title('Model Loss (BMI Categorized)')
plt.legend()

plt.tight_layout()
plt.savefig(os.path.join(OUTPUT_DIR, "training_result_v3_bmi_cat.png"))
plt.show()

# Confusion Matrix
y_pred = model.predict(X_test)
y_pred_classes = np.argmax(y_pred, axis=1)
y_true = np.argmax(y_test, axis=1)

print("\n--- Classification Report ---")
print(classification_report(y_true, y_pred_classes, target_names=CLASSES))

plt.figure(figsize=(8, 6))
cm = confusion_matrix(y_true, y_pred_classes)
sns.heatmap(cm, annot=True, fmt='d', cmap='Greens', xticklabels=CLASSES, yticklabels=CLASSES)
plt.title('Confusion Matrix (V3 - BMI Categorized)')
plt.ylabel('True Label')
plt.xlabel('Predicted Label')
plt.savefig(os.path.join(OUTPUT_DIR, "confusion_matrix_v3_bmi_cat.png"))
plt.show()

model.save(os.path.join(OUTPUT_DIR, "gait_risk_model_v3.h5"))
print(f"\nüíæ ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå! ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÑ‡∏ß‡πâ‡∏ó‡∏µ‡πà: {OUTPUT_DIR}")
